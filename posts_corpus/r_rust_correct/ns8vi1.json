{
    "id": "ns8vi1",
    "title": "Meet Crusty, Fast Scalable && Polite Broad Web Crawler Built With Rust!",
    "selftext": "I've always been fascinated with web crawling and especially broad web crawling - it presents unique set of most interesting challenges...\n\nSo I thought why not learn Rust and make a Broad Web Crawler... Ambitious challenge? I'm in!\n\nCrusty is designed to:\n\n\\- Provide a way to study www and area of broad web crawling in particular\n\n\\- Provide program interfaces for extensibility, configurability and custom data collection\n\n\\- Be uncompromisingly fast with stable && predictable single node performance && decent hardware saturation(rust seems like a natural, almost perfect fit)\n\n\\- Scale with ease(from gprs connection to 10gbit/s port and beyond)\n\n\\- Be polite(probably the most important part of broad web crawling)\n\n\\- Be observable(logs, custom metrics, real-time grafana dashboard)\n\n\\- Be easy to interfact with(build and run just with one command, reproducible docker builds)\n\nYou can check Crusty at [https://github.com/let4be/crusty](https://github.com/let4be/crusty)\n\nWhile I was working on Crusty I realized that the web crawling core is universal and spent some time separating crawling logic into a library [https://github.com/let4be/crusty-core](https://github.com/let4be/crusty-core)\n\nNow Crusty is built on top of Crusty-core which handles all low-level aspects of web crawling while exposing configurability interfaces and ways to fetch the data.\n\nWhile I haven't reached 100% of my goals I think Crusty is at a stage where it's reasonably stable, configurable and usable in general to be experimented with by other people if someone is interested :)\n\nWould love to hear your thoughts!"
}